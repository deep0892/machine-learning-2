{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten\nfrom keras.layers import BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define input image dimensions\n#Large images take too much time and resources.\nimg_rows = 28\nimg_cols = 28\nchannels = 1\nimg_shape = (img_rows, img_cols, channels)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##########################################################################\n#Given input of noise (latent) vector, the Generator produces an image.\ndef build_generator():\n\n    noise_shape = (100,) #1D array of size 100 (latent vector / noise)\n\n#Define your generator network \n#Here we are only using Dense layers. But network can be complicated based\n#on the application. For example, you can use VGG for super res. GAN.         \n\n    model = Sequential()\n\n    model.add(Dense(256, input_shape=noise_shape))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(1024))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    \n    model.add(Dense(np.prod(img_shape), activation='tanh'))\n    model.add(Reshape(img_shape))\n\n    model.summary()\n\n    noise = Input(shape=noise_shape)\n    img = model(noise)    #Generated image\n\n    return Model(noise, img)\n\n#Alpha — α is a hyperparameter which controls the underlying value to which the\n#function saturates negatives network inputs.\n#Momentum — Speed up the training\n##########################################################################","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Given an input image, the Discriminator outputs the likelihood of the image being real.\n    #Binary classification - true or false (we're calling it validity)\n\ndef build_discriminator():\n\n\n    model = Sequential()\n\n    model.add(Flatten(input_shape=img_shape))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(256))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.summary()\n\n    img = Input(shape=img_shape)\n    validity = model(img)\n\n    return Model(img, validity)\n#The validity is the Discriminator’s guess of input being real or not.","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now that we have constructed our two models it’s time to pit them against each other.\n#We do this by defining a training function, loading the data set, re-scaling our training\n#images and setting the ground truths. \ndef train(epochs, batch_size=128, save_interval=50):\n\n    # Load the dataset\n    (X_train, _), (_, _) = mnist.load_data()\n\n    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n\n#Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n    X_train = np.expand_dims(X_train, axis=3) \n\n    half_batch = int(batch_size / 2)\n\n\n#We then loop through a number of epochs to train our Discriminator by first selecting\n#a random batch of images from our true dataset, generating a set of images from our\n#Generator, feeding both set of images into our Discriminator, and finally setting the\n#loss parameters for both the real and fake images, as well as the combined loss. \n    \n    for epoch in range(epochs):\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        # Select a random half batch of real images\n        idx = np.random.randint(0, X_train.shape[0], half_batch)\n        imgs = X_train[idx]\n\n \n        noise = np.random.normal(0, 1, (half_batch, 100))\n\n        # Generate a half batch of fake images\n        gen_imgs = generator.predict(noise)\n\n        # Train the discriminator on real and fake images, separately\n        #Research showed that separate training is more effective. \n        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n    #take average loss from real and fake images. \n    #\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n\n#And within the same loop we train our Generator, by setting the input noise and\n#ultimately training the Generator to have the Discriminator label its samples as valid\n#by specifying the gradient loss.\n        # ---------------------\n        #  Train Generator\n        # ---------------------\n#Create noise vectors as input for generator. \n#Create as many noise vectors as defined by the batch size. \n#Based on normal distribution. Output will be of size (batch size, 100)\n        noise = np.random.normal(0, 1, (batch_size, 100)) \n\n        # The generator wants the discriminator to label the generated samples\n        # as valid (ones)\n        #This is where the genrator is trying to trick discriminator into believing\n        #the generated image is true (hence value of 1 for y)\n        valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n\n        # Generator is part of combined where it got directly linked with the discriminator\n        # Train the generator with noise as x and 1 as y. \n        # Again, 1 as the output as it is adversarial and if generator did a great\n        #job of folling the discriminator then the output would be 1 (true)\n        g_loss = combined.train_on_batch(noise, valid_y)\n\n\n#Additionally, in order for us to keep track of our training process, we print the\n#progress and save the sample image output depending on the epoch interval specified.  \n# Plot the progress\n        \n        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n\n        # If at save interval => save generated image samples\n        if epoch % save_interval == 0:\n            save_imgs(epoch)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#when the specific sample_interval is hit, we call the\n#sample_image function. Which looks as follows.\n\ndef save_imgs(epoch):\n    r, c = 5, 5\n    noise = np.random.normal(0, 1, (r * c, 100))\n    gen_imgs = generator.predict(noise)\n\n    # Rescale images 0 - 1\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    fig, axs = plt.subplots(r, c)\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n            axs[i,j].axis('off')\n            cnt += 1\n    fig.savefig(\"/kaggle/working/mnist_%d.png\" % epoch)\n    plt.close()\n#This function saves our images for us to view","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n\n#Let us also define our optimizer for easy use later on.\n#That way if you change your mind, you can change it easily here\noptimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build and compile the discriminator first. \n#Generator will be trained as part of the combined model, later. \n#pick the loss function and the type of metric to keep track.                 \n#Binary cross entropy as we are doing prediction and it is a better\n#loss function compared to MSE or other. \ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy',\n    optimizer=optimizer,\n    metrics=['accuracy'])","execution_count":9,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               401920    \n_________________________________________________________________\nleaky_re_lu (LeakyReLU)      (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               131328    \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 257       \n=================================================================\nTotal params: 533,505\nTrainable params: 533,505\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build and compile our Discriminator, pick the loss function\n\n#SInce we are only generating (faking) images, let us not track any metrics.\ngenerator = build_generator()\ngenerator.compile(loss='binary_crossentropy', optimizer=optimizer)","execution_count":10,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              (None, 256)               25856     \n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 256)               1024      \n_________________________________________________________________\ndense_4 (Dense)              (None, 512)               131584    \n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 512)               2048      \n_________________________________________________________________\ndense_5 (Dense)              (None, 1024)              525312    \n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 1024)              4096      \n_________________________________________________________________\ndense_6 (Dense)              (None, 784)               803600    \n_________________________________________________________________\nreshape (Reshape)            (None, 28, 28, 1)         0         \n=================================================================\nTotal params: 1,493,520\nTrainable params: 1,489,936\nNon-trainable params: 3,584\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##This builds the Generator and defines the input noise. \n#In a GAN the Generator network takes noise z as an input to produce its images.  \nz = Input(shape=(100,))   #Our random input to the generator\nimg = generator(z)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This ensures that when we combine our networks we only train the Generator.\n#While generator training we do not want discriminator weights to be adjusted. \n#This Doesn't affect the above descriminator training.     \ndiscriminator.trainable = False ","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This specifies that our Discriminator will take the images generated by our Generator\n#and true dataset and set its output to a parameter called valid, which will indicate\n#whether the input is real or not.  \nvalid = discriminator(img)  #Validity check on the generated image","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we combined the models and also set our loss function and optimizer. \n#Again, we are only training the generator here. \n#The ultimate goal here is for the Generator to fool the Discriminator.  \n# The combined model  (stacked generator and discriminator) takes\n# noise as input => generates images => determines validity\n\ncombined = Model(z, valid)\ncombined.compile(loss='binary_crossentropy', optimizer=optimizer)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(epochs=20009, batch_size=32, save_interval=100)","execution_count":15,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n0 [D loss: 1.128274, acc.: 40.62%] [G loss: 0.928029]\n1 [D loss: 0.434559, acc.: 96.88%] [G loss: 0.892308]\n2 [D loss: 0.329259, acc.: 96.88%] [G loss: 0.920689]\n3 [D loss: 0.289867, acc.: 96.88%] [G loss: 1.017648]\n4 [D loss: 0.289913, acc.: 90.62%] [G loss: 1.020821]\n5 [D loss: 0.275205, acc.: 93.75%] [G loss: 1.174161]\n6 [D loss: 0.246624, acc.: 100.00%] [G loss: 1.257960]\n7 [D loss: 0.209700, acc.: 100.00%] [G loss: 1.376095]\n8 [D loss: 0.168811, acc.: 100.00%] [G loss: 1.472344]\n9 [D loss: 0.147490, acc.: 96.88%] [G loss: 1.529235]\n10 [D loss: 0.142939, acc.: 100.00%] [G loss: 1.789845]\n11 [D loss: 0.106052, acc.: 100.00%] [G loss: 1.849683]\n12 [D loss: 0.132506, acc.: 100.00%] [G loss: 2.080273]\n13 [D loss: 0.087903, acc.: 100.00%] [G loss: 2.158487]\n14 [D loss: 0.077600, acc.: 100.00%] [G loss: 2.258468]\n15 [D loss: 0.091327, acc.: 100.00%] [G loss: 2.389393]\n16 [D loss: 0.064010, acc.: 100.00%] [G loss: 2.316881]\n17 [D loss: 0.069440, acc.: 100.00%] [G loss: 2.560833]\n18 [D loss: 0.055301, acc.: 100.00%] [G loss: 2.558440]\n19 [D loss: 0.059868, acc.: 100.00%] [G loss: 2.681245]\n20 [D loss: 0.049736, acc.: 100.00%] [G loss: 2.659140]\n21 [D loss: 0.052358, acc.: 100.00%] [G loss: 2.748079]\n22 [D loss: 0.056363, acc.: 100.00%] [G loss: 2.753208]\n23 [D loss: 0.057714, acc.: 100.00%] [G loss: 2.829456]\n24 [D loss: 0.037376, acc.: 100.00%] [G loss: 2.900388]\n25 [D loss: 0.030792, acc.: 100.00%] [G loss: 2.909262]\n26 [D loss: 0.046389, acc.: 100.00%] [G loss: 3.012299]\n27 [D loss: 0.036417, acc.: 100.00%] [G loss: 3.104115]\n28 [D loss: 0.025596, acc.: 100.00%] [G loss: 3.044464]\n29 [D loss: 0.029819, acc.: 100.00%] [G loss: 3.035873]\n30 [D loss: 0.040236, acc.: 100.00%] [G loss: 3.120167]\n31 [D loss: 0.033583, acc.: 100.00%] [G loss: 3.120698]\n32 [D loss: 0.039480, acc.: 100.00%] [G loss: 3.312970]\n33 [D loss: 0.032040, acc.: 100.00%] [G loss: 3.203959]\n34 [D loss: 0.019715, acc.: 100.00%] [G loss: 3.334649]\n35 [D loss: 0.027790, acc.: 100.00%] [G loss: 3.339026]\n36 [D loss: 0.023071, acc.: 100.00%] [G loss: 3.414572]\n37 [D loss: 0.028636, acc.: 100.00%] [G loss: 3.340024]\n38 [D loss: 0.023216, acc.: 100.00%] [G loss: 3.297900]\n39 [D loss: 0.025459, acc.: 100.00%] [G loss: 3.570545]\n40 [D loss: 0.020473, acc.: 100.00%] [G loss: 3.474562]\n41 [D loss: 0.024221, acc.: 100.00%] [G loss: 3.507813]\n42 [D loss: 0.036245, acc.: 100.00%] [G loss: 3.669089]\n43 [D loss: 0.019368, acc.: 100.00%] [G loss: 3.685554]\n44 [D loss: 0.020898, acc.: 100.00%] [G loss: 3.540098]\n45 [D loss: 0.021940, acc.: 100.00%] [G loss: 3.697886]\n46 [D loss: 0.021127, acc.: 100.00%] [G loss: 3.709800]\n47 [D loss: 0.020509, acc.: 100.00%] [G loss: 3.693237]\n48 [D loss: 0.020417, acc.: 100.00%] [G loss: 3.558425]\n49 [D loss: 0.014565, acc.: 100.00%] [G loss: 3.705462]\n50 [D loss: 0.017450, acc.: 100.00%] [G loss: 3.830782]\n51 [D loss: 0.040672, acc.: 100.00%] [G loss: 3.897073]\n52 [D loss: 0.014206, acc.: 100.00%] [G loss: 4.099247]\n53 [D loss: 0.017676, acc.: 100.00%] [G loss: 3.937639]\n54 [D loss: 0.018302, acc.: 100.00%] [G loss: 3.822095]\n55 [D loss: 0.015677, acc.: 100.00%] [G loss: 3.833192]\n56 [D loss: 0.018762, acc.: 100.00%] [G loss: 3.884388]\n57 [D loss: 0.012785, acc.: 100.00%] [G loss: 3.880080]\n58 [D loss: 0.014171, acc.: 100.00%] [G loss: 3.751752]\n59 [D loss: 0.009611, acc.: 100.00%] [G loss: 3.924890]\n60 [D loss: 0.020909, acc.: 100.00%] [G loss: 3.838360]\n61 [D loss: 0.025543, acc.: 100.00%] [G loss: 3.886627]\n62 [D loss: 0.014775, acc.: 100.00%] [G loss: 3.968220]\n63 [D loss: 0.011210, acc.: 100.00%] [G loss: 3.902080]\n64 [D loss: 0.008588, acc.: 100.00%] [G loss: 4.031337]\n65 [D loss: 0.018542, acc.: 100.00%] [G loss: 3.933350]\n66 [D loss: 0.019768, acc.: 100.00%] [G loss: 3.906943]\n67 [D loss: 0.011521, acc.: 100.00%] [G loss: 4.040687]\n68 [D loss: 0.014497, acc.: 100.00%] [G loss: 3.997354]\n69 [D loss: 0.015644, acc.: 100.00%] [G loss: 3.957913]\n70 [D loss: 0.017869, acc.: 100.00%] [G loss: 3.954950]\n71 [D loss: 0.013591, acc.: 100.00%] [G loss: 4.051107]\n72 [D loss: 0.018696, acc.: 100.00%] [G loss: 4.150942]\n73 [D loss: 0.023136, acc.: 100.00%] [G loss: 4.292224]\n74 [D loss: 0.016874, acc.: 100.00%] [G loss: 4.251435]\n75 [D loss: 0.014761, acc.: 100.00%] [G loss: 4.220038]\n76 [D loss: 0.022506, acc.: 100.00%] [G loss: 4.340855]\n77 [D loss: 0.026506, acc.: 100.00%] [G loss: 4.323262]\n78 [D loss: 0.009000, acc.: 100.00%] [G loss: 4.531872]\n79 [D loss: 0.014965, acc.: 100.00%] [G loss: 4.412040]\n80 [D loss: 0.014526, acc.: 100.00%] [G loss: 4.346213]\n81 [D loss: 0.019599, acc.: 100.00%] [G loss: 4.235907]\n82 [D loss: 0.014146, acc.: 100.00%] [G loss: 4.447335]\n83 [D loss: 0.015071, acc.: 100.00%] [G loss: 4.602376]\n84 [D loss: 0.015435, acc.: 100.00%] [G loss: 4.605258]\n85 [D loss: 0.015042, acc.: 100.00%] [G loss: 4.554442]\n86 [D loss: 0.009311, acc.: 100.00%] [G loss: 4.453216]\n87 [D loss: 0.009111, acc.: 100.00%] [G loss: 4.397909]\n88 [D loss: 0.027172, acc.: 100.00%] [G loss: 4.537623]\n89 [D loss: 0.012710, acc.: 100.00%] [G loss: 4.589363]\n90 [D loss: 0.011571, acc.: 100.00%] [G loss: 4.524253]\n91 [D loss: 0.010488, acc.: 100.00%] [G loss: 4.772053]\n92 [D loss: 0.005967, acc.: 100.00%] [G loss: 4.464173]\n93 [D loss: 0.010498, acc.: 100.00%] [G loss: 4.433306]\n94 [D loss: 0.007670, acc.: 100.00%] [G loss: 4.424466]\n95 [D loss: 0.011655, acc.: 100.00%] [G loss: 4.446030]\n96 [D loss: 0.010132, acc.: 100.00%] [G loss: 4.390773]\n97 [D loss: 0.019055, acc.: 100.00%] [G loss: 4.544483]\n98 [D loss: 0.022922, acc.: 100.00%] [G loss: 4.671924]\n99 [D loss: 0.011755, acc.: 100.00%] [G loss: 4.745350]\n100 [D loss: 0.012224, acc.: 100.00%] [G loss: 4.686769]\n101 [D loss: 0.021091, acc.: 100.00%] [G loss: 4.551071]\n102 [D loss: 0.013581, acc.: 100.00%] [G loss: 4.669816]\n103 [D loss: 0.020427, acc.: 100.00%] [G loss: 4.794093]\n104 [D loss: 0.007199, acc.: 100.00%] [G loss: 4.620092]\n105 [D loss: 0.014636, acc.: 100.00%] [G loss: 4.618779]\n106 [D loss: 0.009110, acc.: 100.00%] [G loss: 4.867222]\n107 [D loss: 0.014895, acc.: 100.00%] [G loss: 4.681257]\n108 [D loss: 0.020740, acc.: 100.00%] [G loss: 4.773066]\n109 [D loss: 0.015607, acc.: 100.00%] [G loss: 4.682627]\n110 [D loss: 0.026180, acc.: 100.00%] [G loss: 4.694357]\n111 [D loss: 0.017953, acc.: 100.00%] [G loss: 4.978889]\n112 [D loss: 0.010304, acc.: 100.00%] [G loss: 4.626706]\n113 [D loss: 0.011179, acc.: 100.00%] [G loss: 4.714164]\n114 [D loss: 0.015879, acc.: 100.00%] [G loss: 4.501263]\n115 [D loss: 0.022727, acc.: 100.00%] [G loss: 5.005666]\n116 [D loss: 0.035387, acc.: 100.00%] [G loss: 4.577717]\n117 [D loss: 0.014583, acc.: 100.00%] [G loss: 4.090026]\n118 [D loss: 0.056006, acc.: 100.00%] [G loss: 4.831801]\n119 [D loss: 0.013624, acc.: 100.00%] [G loss: 4.956108]\n120 [D loss: 0.067197, acc.: 96.88%] [G loss: 4.206012]\n121 [D loss: 0.034609, acc.: 100.00%] [G loss: 5.157520]\n122 [D loss: 0.063231, acc.: 100.00%] [G loss: 4.352049]\n123 [D loss: 0.007879, acc.: 100.00%] [G loss: 4.403121]\n124 [D loss: 0.043496, acc.: 100.00%] [G loss: 5.288897]\n125 [D loss: 0.487653, acc.: 81.25%] [G loss: 3.250744]\n126 [D loss: 0.153015, acc.: 93.75%] [G loss: 4.198231]\n127 [D loss: 0.197451, acc.: 93.75%] [G loss: 5.657641]\n128 [D loss: 0.018305, acc.: 100.00%] [G loss: 5.734995]\n129 [D loss: 0.385275, acc.: 84.38%] [G loss: 3.785308]\n130 [D loss: 0.245954, acc.: 90.62%] [G loss: 4.031739]\n131 [D loss: 0.028865, acc.: 100.00%] [G loss: 4.472986]\n132 [D loss: 0.043433, acc.: 96.88%] [G loss: 4.661252]\n133 [D loss: 0.068572, acc.: 96.88%] [G loss: 4.683557]\n134 [D loss: 0.027070, acc.: 100.00%] [G loss: 4.583879]\n135 [D loss: 0.059412, acc.: 96.88%] [G loss: 4.519726]\n136 [D loss: 0.083649, acc.: 96.88%] [G loss: 3.909850]\n137 [D loss: 0.095400, acc.: 96.88%] [G loss: 4.301010]\n138 [D loss: 0.132374, acc.: 90.62%] [G loss: 3.861029]\n139 [D loss: 0.092241, acc.: 100.00%] [G loss: 3.919487]\n140 [D loss: 0.168729, acc.: 93.75%] [G loss: 4.325336]\n141 [D loss: 1.137569, acc.: 62.50%] [G loss: 2.729123]\n142 [D loss: 0.370979, acc.: 81.25%] [G loss: 2.820285]\n143 [D loss: 0.095027, acc.: 96.88%] [G loss: 3.692082]\n144 [D loss: 0.061664, acc.: 100.00%] [G loss: 3.558125]\n","name":"stdout"},{"output_type":"stream","text":"145 [D loss: 0.116488, acc.: 96.88%] [G loss: 3.745285]\n146 [D loss: 0.178607, acc.: 93.75%] [G loss: 3.034408]\n147 [D loss: 0.132330, acc.: 93.75%] [G loss: 3.326869]\n148 [D loss: 0.128567, acc.: 96.88%] [G loss: 3.476084]\n149 [D loss: 0.120339, acc.: 96.88%] [G loss: 3.225434]\n150 [D loss: 0.040719, acc.: 100.00%] [G loss: 3.164146]\n151 [D loss: 0.113362, acc.: 96.88%] [G loss: 3.275152]\n152 [D loss: 0.175670, acc.: 93.75%] [G loss: 3.630976]\n153 [D loss: 0.561271, acc.: 68.75%] [G loss: 2.602062]\n154 [D loss: 0.114491, acc.: 96.88%] [G loss: 3.588335]\n155 [D loss: 0.041367, acc.: 100.00%] [G loss: 3.444336]\n156 [D loss: 0.199130, acc.: 93.75%] [G loss: 2.855395]\n157 [D loss: 0.151578, acc.: 96.88%] [G loss: 3.619013]\n158 [D loss: 0.152996, acc.: 93.75%] [G loss: 3.057476]\n159 [D loss: 0.123168, acc.: 100.00%] [G loss: 3.705331]\n160 [D loss: 0.719896, acc.: 75.00%] [G loss: 2.187556]\n161 [D loss: 0.227632, acc.: 87.50%] [G loss: 2.927242]\n162 [D loss: 0.155214, acc.: 90.62%] [G loss: 4.404693]\n163 [D loss: 0.284437, acc.: 87.50%] [G loss: 3.384659]\n164 [D loss: 0.068409, acc.: 100.00%] [G loss: 3.974692]\n165 [D loss: 0.127032, acc.: 93.75%] [G loss: 4.137529]\n166 [D loss: 0.411231, acc.: 81.25%] [G loss: 2.666691]\n167 [D loss: 0.255150, acc.: 87.50%] [G loss: 4.090508]\n168 [D loss: 0.208449, acc.: 87.50%] [G loss: 4.082396]\n169 [D loss: 0.196801, acc.: 93.75%] [G loss: 3.022980]\n170 [D loss: 0.135244, acc.: 87.50%] [G loss: 4.112196]\n171 [D loss: 1.053901, acc.: 53.12%] [G loss: 2.407358]\n172 [D loss: 0.068012, acc.: 96.88%] [G loss: 3.244894]\n173 [D loss: 0.293056, acc.: 87.50%] [G loss: 2.624341]\n174 [D loss: 0.123865, acc.: 96.88%] [G loss: 3.188143]\n175 [D loss: 0.063090, acc.: 100.00%] [G loss: 3.065080]\n176 [D loss: 0.098072, acc.: 100.00%] [G loss: 2.998720]\n177 [D loss: 0.207342, acc.: 93.75%] [G loss: 2.739558]\n178 [D loss: 0.260935, acc.: 81.25%] [G loss: 2.779182]\n179 [D loss: 0.108083, acc.: 96.88%] [G loss: 3.407948]\n180 [D loss: 1.124975, acc.: 59.38%] [G loss: 1.592530]\n181 [D loss: 0.301034, acc.: 84.38%] [G loss: 2.839505]\n182 [D loss: 0.211382, acc.: 96.88%] [G loss: 3.458726]\n183 [D loss: 0.260207, acc.: 90.62%] [G loss: 3.030955]\n184 [D loss: 0.313133, acc.: 81.25%] [G loss: 3.035925]\n185 [D loss: 0.129817, acc.: 100.00%] [G loss: 2.966649]\n186 [D loss: 0.127730, acc.: 100.00%] [G loss: 2.336361]\n187 [D loss: 0.330569, acc.: 75.00%] [G loss: 3.090722]\n188 [D loss: 0.120876, acc.: 100.00%] [G loss: 3.835340]\n189 [D loss: 0.268643, acc.: 90.62%] [G loss: 2.353977]\n190 [D loss: 0.118481, acc.: 96.88%] [G loss: 2.699495]\n191 [D loss: 0.203183, acc.: 90.62%] [G loss: 3.659180]\n192 [D loss: 0.142585, acc.: 93.75%] [G loss: 2.546345]\n193 [D loss: 0.169999, acc.: 93.75%] [G loss: 2.866697]\n194 [D loss: 0.755748, acc.: 71.88%] [G loss: 1.507052]\n195 [D loss: 0.146845, acc.: 93.75%] [G loss: 3.424879]\n196 [D loss: 0.129192, acc.: 96.88%] [G loss: 2.425036]\n197 [D loss: 0.202062, acc.: 90.62%] [G loss: 2.913299]\n198 [D loss: 0.273252, acc.: 90.62%] [G loss: 2.657409]\n199 [D loss: 0.242067, acc.: 87.50%] [G loss: 2.875037]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save model for future use to generate fake images\n#Not tested yet... make sure right model is being saved..\n#Compare with GAN4\n\ngenerator.save('generator_model.h5')  #Test the model on GAN4_predict...\n#Change epochs back to 30K\n                \n#Epochs dictate the number of backward and forward propagations, the batch_size\n#indicates the number of training samples per backward/forward propagation, and the\n#sample_interval specifies after how many epochs we call our sample_image function.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}